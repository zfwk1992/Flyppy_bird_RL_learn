#!/usr/bin/env python
from __future__ import print_function

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import cv2
import sys
import game.wrapped_flappy_bird_fast as game  
import random
import numpy as np
from collections import deque
import os
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—è®°å½•
def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    os.makedirs("logs", exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"logs/training_optimized_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logging.info(f"æ—¥å¿—æ–‡ä»¶åˆ›å»º: {log_filename}")
    return log_filename

# æ£€æŸ¥GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    torch.cuda.set_per_process_memory_fraction(0.8)

# ğŸš€ ä¼˜åŒ–ç½‘ç»œé…ç½®å‚æ•°
GAME = 'bird'
ACTIONS = 2
GAMMA = 0.99
OBSERVE = 5000        #è§‚å¯Ÿæ­¥æ•°
EXPLORE = 25000     # æ¢ç´¢æ­¥æ•°
FINAL_EPSILON = 0.001
REPLAY_MEMORY = 20000 # é€‚ä¸­çš„ç»éªŒæ± 
BATCH = 64           # å¢å¤§æ‰¹æ¬¡æé«˜æ•ˆç‡
FRAME_PER_ACTION = 2  # æ¯1å¸§ä¸€æ¬¡åŠ¨ä½œï¼Œå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡

class OptimizedDQN(nn.Module):
    """ä¼˜åŒ–ç‰ˆæ·±åº¦Qç½‘ç»œ - è§£å†³ç»´åº¦é—®é¢˜å¹¶æå‡æ€§èƒ½"""
    def __init__(self, actions):
        super(OptimizedDQN, self).__init__()
        
        # æ›´å°çš„å·ç§¯ç½‘ç»œ
        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)
        
        # æ‰¹å½’ä¸€åŒ–å±‚ - åŠ é€Ÿè®­ç»ƒ
        self.bn1 = nn.BatchNorm2d(32)
        self.bn2 = nn.BatchNorm2d(64)
        
        # è‡ªé€‚åº”æ± åŒ– - ç¡®ä¿è¾“å‡ºå°ºå¯¸å›ºå®š
        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))
        
        # å…¨è¿æ¥å±‚è§„æ¨¡å‡å°
        self.fc1 = nn.Linear(64 * 4 * 4, 1024)  # åªä¿ç•™å·ç§¯è¾“å‡º
        self.fc2 = nn.Linear(1024, actions)

        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """ä¼˜åŒ–çš„æƒé‡åˆå§‹åŒ–"""
        for module in self.modules():
            if isinstance(module, nn.Conv2d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, (nn.BatchNorm2d, nn.BatchNorm1d, nn.LayerNorm)):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        # x: å›¾åƒè¾“å…¥
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.adaptive_pool(x)
        x_flat = x.reshape(x.size(0), -1)
        x = F.relu(self.fc1(x_flat))
        x = self.fc2(x)
        return x

class OptimizedDQNAgent:
    """ä¼˜åŒ–ç‰ˆDQNæ™ºèƒ½ä½“"""
    def __init__(self, actions):
        self.actions = actions
        self.device = device
        
        # åˆ›å»ºä¼˜åŒ–ç½‘ç»œ
        self.q_network = OptimizedDQN(actions).to(device)
        self.target_network = OptimizedDQN(actions).to(device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        # ğŸ¯ ä¼˜åŒ–å™¨é…ç½®
        self.optimizer = optim.AdamW(
            self.q_network.parameters(), 
            lr=2e-4,  # ç¨é«˜çš„å­¦ä¹ ç‡
            weight_decay=1e-4,  # æƒé‡è¡°å‡
            betas=(0.9, 0.999)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä½™å¼¦é€€ç«
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, 
            T_max=EXPLORE, 
            eta_min=1e-6
        )
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.memory = deque(maxlen=REPLAY_MEMORY)
        self.trajectory_buffer = []  # æ–°å¢è½¨è¿¹ç¼“å†²åŒº
        
        # è®­ç»ƒå‚æ•°
        self.epsilon = 1.0
        self.step = 0
        
        # æ€§èƒ½ç›‘æ§
        self.loss_history = []
        self.reward_history = []
        
    def preprocess_state(self, state):
        """ä¼˜åŒ–çš„çŠ¶æ€é¢„å¤„ç†"""
        # è½¬æ¢ä¸ºç°åº¦å›¾
        state = cv2.cvtColor(cv2.resize(state, (80, 80)), cv2.COLOR_BGR2GRAY)
        # äºŒå€¼åŒ–
        _, state = cv2.threshold(state, 1, 255, cv2.THRESH_BINARY)
        # å½’ä¸€åŒ–
        state = state.astype(np.float32) / 255.0
        return state
    
    def get_state_tensor(self, state_stack):
        """å°†çŠ¶æ€å †æ ˆè½¬æ¢ä¸ºtensor"""
        state_tensor = torch.FloatTensor(state_stack).unsqueeze(0).to(device)
        state_tensor = state_tensor.permute(0, 3, 1, 2)
        return state_tensor
    
    def select_action(self, state_tensor):
        """ä¼˜åŒ–çš„åŠ¨ä½œé€‰æ‹©ç­–ç•¥"""
        if self.step <= OBSERVE:
            # è§‚å¯Ÿé˜¶æ®µï¼šçº¯éšæœºåŠ¨ä½œ
            action = random.randrange(self.actions)
            if self.step % 100 == 0:
                logging.info(f"ğŸ‘€ è§‚å¯Ÿé˜¶æ®µéšæœºåŠ¨ä½œ: {action} (Îµ=1.0)")
        elif random.random() <= self.epsilon:
            # æ¢ç´¢é˜¶æ®µï¼šéšæœºåŠ¨ä½œ
            action = random.randrange(self.actions)
            if self.step % 100 == 0:
                logging.info(f"ğŸ² æ¢ç´¢é˜¶æ®µéšæœºåŠ¨ä½œ: {action} (Îµ={self.epsilon:.4f})")
        else:
            # åˆ©ç”¨é˜¶æ®µï¼šç½‘ç»œå†³ç­–
            with torch.no_grad():
                q_values = self.q_network(state_tensor)
                action = q_values.argmax().item()
                if self.step % 100 == 0:
                    logging.info(f"ğŸ§  ç½‘ç»œå†³ç­–: {action} (Qå€¼: {q_values.max().item():.4f})")
        
        return action
    
    def store_transition(self, state, action, reward, next_state, terminal):
        """å­˜å‚¨ç»éªŒ"""
        self.trajectory_buffer.append((state, action, reward, next_state, terminal))
        if len(self.trajectory_buffer) == 50:  # è¿™é‡Œæ”¹ä¸º50
            self.memory.append(list(self.trajectory_buffer))  # å­˜ä¸€æ®µè¿ç»­è½¨è¿¹
            self.trajectory_buffer.pop(0)  # æ»‘åŠ¨çª—å£
        if terminal:
            self.trajectory_buffer.clear()  # æ¸¸æˆç»“æŸæ¸…ç©º
    
    def train(self):
        """ä¼˜åŒ–çš„è®­ç»ƒæ–¹æ³•"""
        if len(self.memory) < BATCH:
            return

        batch = random.sample(self.memory, BATCH)  # batch: [BATCH, 50, 5]

        # åªå–æ¯æ®µè½¨è¿¹çš„é¦–çŠ¶æ€ã€é¦–åŠ¨ä½œã€ç´¯è®¡å¥–åŠ±ã€æœ€åä¸€ä¸ªnext_stateã€æœ€åä¸€ä¸ªterminal
        states = []
        actions = []
        rewards = []
        next_states = []
        terminals = []

        for traj in batch:
            states.append(traj[0][0])          # é¦–çŠ¶æ€
            actions.append(traj[0][1])         # é¦–åŠ¨ä½œ
            # næ­¥ç´¯è®¡å¥–åŠ±
            R = 0
            for i in range(len(traj)):
                R += (GAMMA ** i) * traj[i][2]
            rewards.append(R)
            next_states.append(traj[-1][3])    # æœ€åä¸€æ­¥çš„ next_state
            terminals.append(traj[-1][4])      # æœ€åä¸€æ­¥çš„ terminal

        # è½¬ä¸ºtensor
        states = torch.FloatTensor(states).to(device)
        actions = torch.LongTensor(actions).to(device)
        rewards = torch.FloatTensor(rewards).to(device)
        next_states = torch.FloatTensor(next_states).to(device)
        terminals = torch.BoolTensor(terminals).to(device)

        # è°ƒæ•´ç»´åº¦é¡ºåº
        states = states.permute(0, 3, 1, 2)
        next_states = next_states.permute(0, 3, 1, 2)

        # è®¡ç®—å½“å‰Qå€¼
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))

        # è®¡ç®—ç›®æ ‡Qå€¼ - ä½¿ç”¨Double DQN
        with torch.no_grad():
            next_actions = self.q_network(next_states).argmax(1)
            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1))
            target_q_values = rewards + (GAMMA ** len(traj)) * next_q_values.squeeze() * ~terminals

        loss = F.huber_loss(current_q_values.squeeze(), target_q_values)

        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)
        self.optimizer.step()
        self.scheduler.step()
        self.loss_history.append(loss.item())
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return loss.item()
    
    def update_target_network(self):
        """è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        tau = 0.001  # è½¯æ›´æ–°å‚æ•°
        for target_param, local_param in zip(self.target_network.parameters(), self.q_network.parameters()):
            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)
    
    def update_epsilon(self):
        """æ›´æ–°æ¢ç´¢ç‡"""
        if self.step <= OBSERVE:
            self.epsilon = 1.0
        elif self.epsilon > FINAL_EPSILON and self.step > OBSERVE:
            self.epsilon -= (1.0 - FINAL_EPSILON) / EXPLORE
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'epsilon': self.epsilon,
            'step': self.step,
            'loss_history': self.loss_history,
            'reward_history': self.reward_history
        }, path)
    
    def load_model(self, path):
        """åŠ è½½æ¨¡å‹"""
        if os.path.exists(path):
            checkpoint = torch.load(path, map_location=device)
            self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
            self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            self.epsilon = checkpoint['epsilon']
            self.step = checkpoint['step']
            self.loss_history = checkpoint.get('loss_history', [])
            self.reward_history = checkpoint.get('reward_history', [])
            logging.info(f"æˆåŠŸåŠ è½½æ¨¡å‹: {path}")
            return True
        else:
            logging.warning("æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹")
            return False

def train_network():
    """è®­ç»ƒç½‘ç»œ"""
    # è®¾ç½®æ—¥å¿—è®°å½•
    log_filename = setup_logging()
    logging.info("ğŸš€ å¼€å§‹Flappy Bird AIä¼˜åŒ–ç‰ˆç½‘ç»œè®­ç»ƒ")
    logging.info("=" * 60)
    
    # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
    logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        logging.info(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
        logging.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        logging.info(f"å½“å‰GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB")
    else:
        logging.warning("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    
    # æ˜¾ç¤ºä¼˜åŒ–ç‰ˆç½‘ç»œé…ç½®å‚æ•°
    logging.info("âš¡ ä¼˜åŒ–ç‰ˆç½‘ç»œé…ç½®å‚æ•°:")
    logging.info(f"   - FRAME_PER_ACTION: {FRAME_PER_ACTION}")
    logging.info(f"   - OBSERVE: {OBSERVE}")
    logging.info(f"   - EXPLORE: {EXPLORE}")
    logging.info(f"   - BATCH: {BATCH}")
    logging.info(f"   - ç½‘ç»œæ¶æ„: ä¼˜åŒ–ç‰ˆ (è‡ªé€‚åº”æ± åŒ– + æ®‹å·®è¿æ¥ + Double DQN)")
    logging.info(f"   - ä¼˜åŒ–å™¨: AdamW + ä½™å¼¦é€€ç«å­¦ä¹ ç‡")
    logging.info(f"   - æŸå¤±å‡½æ•°: Huber Loss")
    logging.info(f"   - é¢„æœŸé€Ÿåº¦: ~40æ­¥/ç§’")
    logging.info(f"   - é¢„æœŸæ—¶é—´: ~12åˆ†é’Ÿ")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = OptimizedDQNAgent(ACTIONS)
    
    # éªŒè¯æ¨¡å‹æ˜¯å¦åœ¨GPUä¸Š
    if torch.cuda.is_available():
        q_network_device = next(agent.q_network.parameters()).device
        target_network_device = next(agent.target_network.parameters()).device
        logging.info(f"Qç½‘ç»œè®¾å¤‡: {q_network_device}")
        logging.info(f"ç›®æ ‡ç½‘ç»œè®¾å¤‡: {target_network_device}")
        
        if q_network_device.type != 'cuda' or target_network_device.type != 'cuda':
            logging.error("âŒ æ¨¡å‹æœªæ­£ç¡®ç§»åˆ°GPUï¼")
            agent.q_network = agent.q_network.to(device)
            agent.target_network = agent.target_network.to(device)
            logging.info("âœ… å·²å¼ºåˆ¶å°†æ¨¡å‹ç§»åˆ°GPU")
    
    # åˆ›å»ºæ¸¸æˆç¯å¢ƒ
    game_state = game.GameState()
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    os.makedirs(f"logs_{GAME}", exist_ok=True)
    a_file = open(f"logs_{GAME}/readout.txt", 'w')
    h_file = open(f"logs_{GAME}/hidden.txt", 'w')
    
    # åˆå§‹åŒ–çŠ¶æ€
    do_nothing = np.zeros(ACTIONS)
    do_nothing[0] = 1
    x_t, r_0, terminal = game_state.frame_step(do_nothing)
    x_t = agent.preprocess_state(x_t)
    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)
    
    # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model_path = "saved_networks/bird-dqn-pytorch-optimized.pth"
    agent.load_model(model_path)
    
    # å¸§è®¡æ•°å™¨
    frame_count = 0
    episode_reward = 0
    episode_count = 0
    
    # è®­ç»ƒå¾ªç¯
    while "flappy bird" != "angry bird":
        # è·å–çŠ¶æ€tensor
        state_tensor = agent.get_state_tensor(s_t)
        
        # æ¯FRAME_PER_ACTIONå¸§é‡‡å–ä¸€æ¬¡åŠ¨ä½œ
        if frame_count % FRAME_PER_ACTION == 0:
            # é€‰æ‹©åŠ¨ä½œ
            action_index = agent.select_action(state_tensor)
            a_t = np.zeros([ACTIONS])
            a_t[action_index] = 1
        else:
            # å…¶ä»–å¸§ä¿æŒä¸Šä¸€ä¸ªåŠ¨ä½œ
            a_t = np.zeros([ACTIONS])
            a_t[action_index] = 1
        
        # æ‰§è¡ŒåŠ¨ä½œ
        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)
        x_t1 = agent.preprocess_state(x_t1_colored)
        x_t1 = np.reshape(x_t1, (80, 80, 1))
        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)
        
        # ç´¯ç§¯å¥–åŠ±
        episode_reward += r_t
        
        # å­˜å‚¨ç»éªŒï¼ˆåªåœ¨é‡‡å–åŠ¨ä½œçš„å¸§å­˜å‚¨ï¼‰
        if frame_count % FRAME_PER_ACTION == 0:
            agent.store_transition(s_t, action_index, r_t, s_t1, terminal)
            agent.step += 1
        
        # æ›´æ–°çŠ¶æ€
        s_t = s_t1
        frame_count += 1
        
        # åªåœ¨é‡‡å–åŠ¨ä½œçš„å¸§è¿›è¡Œè®­ç»ƒå’Œæ—¥å¿—è®°å½•
        if frame_count % FRAME_PER_ACTION == 0:
            # è®­ç»ƒç½‘ç»œ
            loss = None
            if agent.step > OBSERVE:
                loss = agent.train()
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆæ¯æ­¥æ›´æ–°ï¼‰
            if agent.step > OBSERVE:
                agent.update_target_network()
            
            # æ›´æ–°æ¢ç´¢ç‡
            agent.update_epsilon()
            
            # ä¿å­˜æ¨¡å‹ï¼ˆæ¯5000æ­¥ï¼‰
            if agent.step % 5000 == 0:
                os.makedirs("saved_networks", exist_ok=True)
                agent.save_model(f"saved_networks/{GAME}-dqn-pytorch-optimized-{agent.step}.pth")
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯
            if agent.step % 100 == 0:
                state = ""
                if agent.step <= OBSERVE:
                    state = "observe"
                elif agent.step > OBSERVE and agent.step <= OBSERVE + EXPLORE:
                    state = "explore"
                else:
                    state = "train"
                
                q_max = agent.q_network(state_tensor).max().item() if loss is not None else 0
                current_lr = agent.scheduler.get_last_lr()[0]
                
                logging.info(f"â±ï¸  TIMESTEP {agent.step} / FRAME {frame_count} / STATE {state} / EPSILON {agent.epsilon:.4f} / "
                      f"ACTION {action_index} / REWARD {r_t} / Q_MAX {q_max:.4f} / LR {current_lr:.2e}")
                
                if loss is not None:
                    logging.info(f"ğŸ“‰ LOSS: {loss:.6f}")
            
            # æ¯1000æ­¥æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if agent.step % 1000 == 0:
                avg_loss = np.mean(agent.loss_history[-100:]) if agent.loss_history else 0
                logging.info(f"ğŸ“Š ç»Ÿè®¡: ç»éªŒæ± å¤§å°={len(agent.memory)}, æ¢ç´¢ç‡={agent.epsilon:.4f}, å¹³å‡æŸå¤±={avg_loss:.6f}")
                
                # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.memory_allocated(0) / 1024**2
                    gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    gpu_memory_reserved = torch.cuda.memory_reserved(0) / 1024**2
                    logging.info(f"ğŸ–¥ï¸  GPUå†…å­˜: {gpu_memory:.1f} MB / {gpu_memory_reserved:.1f} MB / {gpu_memory_total:.1f} GB")
                
                logging.info("-" * 50)
        
        # æ¸¸æˆç»“æŸå¤„ç†
        if terminal:
            episode_count += 1
            agent.reward_history.append(episode_reward)
            episode_reward = 0

def play_game():
    """ä¸»å‡½æ•°"""
    train_network()

if __name__ == "__main__":
    play_game()