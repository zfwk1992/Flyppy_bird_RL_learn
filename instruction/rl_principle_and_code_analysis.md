# ğŸ§  å¼ºåŒ–å­¦ä¹ åŸç†ä¸æ·±åº¦Qç½‘ç»œä»£ç è§£æ

## ğŸ“š å¼ºåŒ–å­¦ä¹ åŸºç¡€åŸç†

### 1. å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µ

#### ğŸ¯ åŸºæœ¬è¦ç´ 
- **æ™ºèƒ½ä½“(Agent)**: å­¦ä¹ å¹¶åšå‡ºå†³ç­–çš„ä¸»ä½“
- **ç¯å¢ƒ(Environment)**: æ™ºèƒ½ä½“äº¤äº’çš„å¤–éƒ¨ä¸–ç•Œ
- **çŠ¶æ€(State)**: ç¯å¢ƒåœ¨æŸä¸€æ—¶åˆ»çš„å®Œæ•´æè¿°
- **åŠ¨ä½œ(Action)**: æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œçš„æ“ä½œ
- **å¥–åŠ±(Reward)**: ç¯å¢ƒå¯¹æ™ºèƒ½ä½“åŠ¨ä½œçš„åé¦ˆ
- **ç­–ç•¥(Policy)**: ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„å‡½æ•°

#### ğŸ”„ äº¤äº’è¿‡ç¨‹
```
æ™ºèƒ½ä½“è§‚å¯ŸçŠ¶æ€ â†’ é€‰æ‹©åŠ¨ä½œ â†’ ç¯å¢ƒåé¦ˆå¥–åŠ± â†’ è½¬ç§»åˆ°æ–°çŠ¶æ€
```

### 2. Q-LearningåŸç†

#### ğŸ¯ Qå€¼æ¦‚å¿µ
Qå€¼è¡¨ç¤ºåœ¨çŠ¶æ€sä¸‹æ‰§è¡ŒåŠ¨ä½œaçš„é•¿æœŸä»·å€¼ï¼š
```
Q(s,a) = å½“å‰å¥–åŠ± + Î³ Ã— æœªæ¥æœ€å¤§Qå€¼
```

#### ğŸ“Š Q-Learningæ›´æ–°å…¬å¼
```
Q(s,a) â† Q(s,a) + Î±[r + Î³ Ã— max Q(s',a') - Q(s,a)]
```
å…¶ä¸­ï¼š
- Î±: å­¦ä¹ ç‡
- r: å³æ—¶å¥–åŠ±
- Î³: æŠ˜æ‰£å› å­
- s': ä¸‹ä¸€ä¸ªçŠ¶æ€
- a': ä¸‹ä¸€ä¸ªåŠ¨ä½œ

### 3. æ·±åº¦Qç½‘ç»œ(DQN)åŸç†

#### ğŸ§  æ ¸å¿ƒæ€æƒ³
ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿‘ä¼¼Qå‡½æ•°ï¼š
```
Q(s,a) â‰ˆ Q(s,a; Î¸)
```
å…¶ä¸­Î¸æ˜¯ç½‘ç»œå‚æ•°ã€‚

#### ğŸ¯ ç›®æ ‡å‡½æ•°
```
L(Î¸) = E[(r + Î³ Ã— max Q(s',a'; Î¸') - Q(s,a; Î¸))Â²]
```
å…¶ä¸­Î¸'æ˜¯ç›®æ ‡ç½‘ç»œå‚æ•°ã€‚

## ğŸ” ä»£ç è¯¦ç»†è§£æ

### 1. ç½‘ç»œæ¶æ„è§£æ

#### ğŸ—ï¸ OptimizedDQNç±»
```python
class OptimizedDQN(nn.Module):
    def __init__(self, actions):
        # å·ç§¯å±‚è®¾è®¡
        self.conv1 = nn.Conv2d(4, 64, kernel_size=8, stride=4, padding=2)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)
        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)
        
        # è‡ªé€‚åº”æ± åŒ– - è§£å†³ç»´åº¦é—®é¢˜
        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(128 * 4 * 4, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, actions)
```

**è®¾è®¡åŸç†**:
- **4é€šé“è¾“å…¥**: 4å¸§æ¸¸æˆç”»é¢å †å ï¼Œæ•æ‰æ—¶é—´ä¿¡æ¯
- **å·ç§¯å±‚**: æå–ç©ºé—´ç‰¹å¾ï¼Œè¯†åˆ«æ¸¸æˆå¯¹è±¡
- **è‡ªé€‚åº”æ± åŒ–**: ç¡®ä¿è¾“å‡ºå°ºå¯¸å›ºå®šï¼Œé¿å…ç»´åº¦é—®é¢˜
- **å…¨è¿æ¥å±‚**: å°†ç‰¹å¾æ˜ å°„åˆ°Qå€¼

#### ğŸ”„ å‰å‘ä¼ æ’­è¿‡ç¨‹
```python
def forward(self, x):
    # 1. å·ç§¯ç‰¹å¾æå–
    x = F.relu(self.bn1(self.conv1(x)))  # 4x80x80 â†’ 64x21x21
    x = F.max_pool2d(x, 2, 2)           # 64x21x21 â†’ 64x10x10
    x = F.relu(self.bn2(self.conv2(x)))  # 64x10x10 â†’ 128x5x5
    x = F.relu(self.bn3(self.conv3(x)))  # 128x5x5 â†’ 128x5x5
    
    # 2. è‡ªé€‚åº”æ± åŒ– - å›ºå®šè¾“å‡ºå°ºå¯¸
    x = self.adaptive_pool(x)            # 128x5x5 â†’ 128x4x4
    
    # 3. å±•å¹³
    x_flat = x.view(x.size(0), -1)      # 128x4x4 â†’ 2048
    
    # 4. å…¨è¿æ¥å±‚ + æ®‹å·®è¿æ¥
    x = F.relu(self.bn_fc1(self.fc1(x_flat)))  # 2048 â†’ 1024
    x = self.dropout1(x)
    x = F.relu(self.bn_fc2(self.fc2(x)))       # 1024 â†’ 512
    x = self.dropout2(x)
    
    # 5. æ®‹å·®è¿æ¥
    residual = F.relu(self.residual(x_flat))   # 2048 â†’ 256
    x = F.relu(self.bn_fc3(self.fc3(x)))       # 512 â†’ 256
    x = self.dropout3(x)
    x = x + residual                            # æ®‹å·®è¿æ¥
    
    # 6. è¾“å‡ºQå€¼
    x = self.fc4(x)                            # 256 â†’ 2 (åŠ¨ä½œæ•°)
    return x
```

### 2. æ™ºèƒ½ä½“è§£æ

#### ğŸ¤– OptimizedDQNAgentç±»
```python
class OptimizedDQNAgent:
    def __init__(self, actions):
        # åˆ›å»ºä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.q_network = OptimizedDQN(actions).to(device)
        self.target_network = OptimizedDQN(actions).to(device)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.memory = deque(maxlen=REPLAY_MEMORY)
        
        # è®­ç»ƒå‚æ•°
        self.epsilon = 1.0  # æ¢ç´¢ç‡
        self.step = 0
```

**æ ¸å¿ƒç»„ä»¶**:
- **ä¸»ç½‘ç»œ**: ç”¨äºåŠ¨ä½œé€‰æ‹©å’Œè®­ç»ƒ
- **ç›®æ ‡ç½‘ç»œ**: æä¾›ç¨³å®šçš„å­¦ä¹ ç›®æ ‡
- **ç»éªŒå›æ”¾**: å­˜å‚¨å’Œé‡ç”¨å†å²ç»éªŒ
- **æ¢ç´¢ç‡**: æ§åˆ¶æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡

#### ğŸ¯ åŠ¨ä½œé€‰æ‹©ç­–ç•¥
```python
def select_action(self, state_tensor):
    if self.step <= OBSERVE:
        # è§‚å¯Ÿé˜¶æ®µï¼šçº¯éšæœºåŠ¨ä½œ
        action = random.randrange(self.actions)
    elif random.random() <= self.epsilon:
        # æ¢ç´¢é˜¶æ®µï¼šéšæœºåŠ¨ä½œ
        action = random.randrange(self.actions)
    else:
        # åˆ©ç”¨é˜¶æ®µï¼šç½‘ç»œå†³ç­–
        with torch.no_grad():
            q_values = self.q_network(state_tensor)
            action = q_values.argmax().item()
    return action
```

**ç­–ç•¥è§£æ**:
1. **è§‚å¯Ÿé˜¶æ®µ**: çº¯éšæœºæ”¶é›†ç»éªŒ
2. **æ¢ç´¢é˜¶æ®µ**: Îµ-è´ªå©ªç­–ç•¥ï¼Œå¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
3. **åˆ©ç”¨é˜¶æ®µ**: ä½¿ç”¨ç½‘ç»œé¢„æµ‹çš„æœ€ä¼˜åŠ¨ä½œ

#### ğŸ§  è®­ç»ƒè¿‡ç¨‹è§£æ
```python
def train(self):
    # 1. éšæœºé‡‡æ ·ç»éªŒæ‰¹æ¬¡
    batch = random.sample(self.memory, BATCH)
    
    # 2. åˆ†ç¦»æ‰¹æ¬¡æ•°æ®
    states = torch.FloatTensor([d[0] for d in batch]).to(device)
    actions = torch.LongTensor([d[1] for d in batch]).to(device)
    rewards = torch.FloatTensor([d[2] for d in batch]).to(device)
    next_states = torch.FloatTensor([d[3] for d in batch]).to(device)
    terminals = torch.BoolTensor([d[4] for d in batch]).to(device)
    
    # 3. è®¡ç®—å½“å‰Qå€¼
    current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
    
    # 4. è®¡ç®—ç›®æ ‡Qå€¼ (Double DQN)
    with torch.no_grad():
        # ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
        next_actions = self.q_network(next_states).argmax(1)
        # ä½¿ç”¨ç›®æ ‡ç½‘ç»œè®¡ç®—Qå€¼
        next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1))
        target_q_values = rewards + (GAMMA * next_q_values.squeeze() * ~terminals)
    
    # 5. è®¡ç®—æŸå¤±
    loss = F.huber_loss(current_q_values.squeeze(), target_q_values)
    
    # 6. åå‘ä¼ æ’­
    self.optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)
    self.optimizer.step()
    
    return loss.item()
```

**è®­ç»ƒæ­¥éª¤è§£æ**:
1. **ç»éªŒé‡‡æ ·**: éšæœºé€‰æ‹©å†å²ç»éªŒï¼Œæ‰“ç ´ç›¸å…³æ€§
2. **Qå€¼è®¡ç®—**: ä¸»ç½‘ç»œè®¡ç®—å½“å‰çŠ¶æ€çš„Qå€¼
3. **ç›®æ ‡è®¡ç®—**: Double DQNå‡å°‘è¿‡ä¼°è®¡
4. **æŸå¤±è®¡ç®—**: HuberæŸå¤±å¯¹å¼‚å¸¸å€¼æ›´é²æ£’
5. **å‚æ•°æ›´æ–°**: æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

### 3. å…³é”®æŠ€æœ¯è§£æ

#### ğŸ”„ ç»éªŒå›æ”¾æœºåˆ¶
```python
def store_transition(self, state, action, reward, next_state, terminal):
    self.memory.append((state, action, reward, next_state, terminal))
```

**ä½œç”¨**:
- **æ‰“ç ´ç›¸å…³æ€§**: éšæœºé‡‡æ ·å‡å°‘è¿ç»­çŠ¶æ€çš„ç›¸å…³æ€§
- **æé«˜æ•ˆç‡**: é‡å¤åˆ©ç”¨ç»éªŒæ•°æ®
- **ç¨³å®šè®­ç»ƒ**: é¿å…è¿ç»­ç›¸ä¼¼æ ·æœ¬çš„å½±å“

#### ğŸ¯ Double DQNæŠ€æœ¯
```python
# ä½¿ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
next_actions = self.q_network(next_states).argmax(1)
# ä½¿ç”¨ç›®æ ‡ç½‘ç»œè®¡ç®—Qå€¼
next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1))
```

**ä¼˜åŠ¿**:
- **å‡å°‘è¿‡ä¼°è®¡**: åˆ†ç¦»åŠ¨ä½œé€‰æ‹©å’ŒQå€¼è®¡ç®—
- **æé«˜ç¨³å®šæ€§**: é¿å…Qå€¼è¿‡åº¦ä¹è§‚
- **æ”¹å–„æ€§èƒ½**: æ›´å‡†ç¡®çš„Qå€¼ä¼°è®¡

#### ğŸ”„ è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
```python
def update_target_network(self):
    tau = 0.001  # è½¯æ›´æ–°å‚æ•°
    for target_param, local_param in zip(self.target_network.parameters(), self.q_network.parameters()):
        target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)
```

**ä¼˜åŠ¿**:
- **ç¨³å®šç›®æ ‡**: ç›®æ ‡ç½‘ç»œå‚æ•°ç¼“æ…¢æ›´æ–°
- **å‡å°‘æŒ¯è¡**: é¿å…Qå€¼ä¼°è®¡çš„å‰§çƒˆå˜åŒ–
- **è¿ç»­æ›´æ–°**: æ¯æ­¥éƒ½è¿›è¡Œè½¯æ›´æ–°

### 4. è®­ç»ƒå¾ªç¯è§£æ

#### ğŸ”„ ä¸»è®­ç»ƒå¾ªç¯
```python
def train_network():
    # åˆå§‹åŒ–
    agent = OptimizedDQNAgent(ACTIONS)
    game_state = game.GameState()
    
    # è®­ç»ƒå¾ªç¯
    while True:
        # 1. è·å–çŠ¶æ€
        state_tensor = agent.get_state_tensor(s_t)
        
        # 2. é€‰æ‹©åŠ¨ä½œ
        if frame_count % FRAME_PER_ACTION == 0:
            action_index = agent.select_action(state_tensor)
        
        # 3. æ‰§è¡ŒåŠ¨ä½œ
        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)
        
        # 4. å­˜å‚¨ç»éªŒ
        agent.store_transition(s_t, action_index, r_t, s_t1, terminal)
        
        # 5. è®­ç»ƒç½‘ç»œ
        if agent.step > OBSERVE:
            loss = agent.train()
        
        # 6. æ›´æ–°ç›®æ ‡ç½‘ç»œ
        agent.update_target_network()
        
        # 7. æ›´æ–°æ¢ç´¢ç‡
        agent.update_epsilon()
```

**è®­ç»ƒé˜¶æ®µ**:
1. **è§‚å¯Ÿé˜¶æ®µ** (0-2000æ­¥): çº¯éšæœºåŠ¨ä½œï¼Œæ”¶é›†ç»éªŒ
2. **æ¢ç´¢é˜¶æ®µ** (2000-22000æ­¥): Îµä»1.0é€æ¸é™ä½åˆ°0.001
3. **è®­ç»ƒé˜¶æ®µ** (22000+æ­¥): ä¸»è¦ä½¿ç”¨ç½‘ç»œå†³ç­–

### 5. ä¼˜åŒ–æŠ€æœ¯è§£æ

#### ğŸ¯ è‡ªé€‚åº”æ± åŒ–
```python
self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))
```
**ä½œç”¨**: ç¡®ä¿è¾“å‡ºå°ºå¯¸å›ºå®šä¸º4x4ï¼Œé¿å…ç»´åº¦è®¡ç®—é”™è¯¯

#### ğŸ”— æ®‹å·®è¿æ¥
```python
residual = F.relu(self.residual(x_flat))
x = x + residual
```
**ä½œç”¨**: æ”¹å–„æ¢¯åº¦æµåŠ¨ï¼ŒåŠ é€Ÿè®­ç»ƒæ”¶æ•›

#### ğŸ“Š HuberæŸå¤±
```python
loss = F.huber_loss(current_q_values.squeeze(), target_q_values)
```
**ä¼˜åŠ¿**: å¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼Œè®­ç»ƒæ›´ç¨³å®š

#### ğŸ¯ AdamWä¼˜åŒ–å™¨
```python
self.optimizer = optim.AdamW(
    self.q_network.parameters(), 
    lr=2e-4,
    weight_decay=1e-4,
    betas=(0.9, 0.999)
)
```
**ä¼˜åŠ¿**: æ›´å¥½çš„æƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

## ğŸ“ˆ å¼ºåŒ–å­¦ä¹ åœ¨Flappy Birdä¸­çš„åº”ç”¨

### ğŸ® çŠ¶æ€è¡¨ç¤º
- **4å¸§å †å **: æ•æ‰å°é¸Ÿçš„è¿åŠ¨è½¨è¿¹
- **80x80å›¾åƒ**: æ¸¸æˆç”»é¢çš„ç®€åŒ–è¡¨ç¤º
- **ç°åº¦äºŒå€¼åŒ–**: çªå‡ºé‡è¦ç‰¹å¾

### ğŸ¯ åŠ¨ä½œç©ºé—´
- **åŠ¨ä½œ0**: ä¸è·³è·ƒ
- **åŠ¨ä½œ1**: è·³è·ƒ

### ğŸ† å¥–åŠ±è®¾è®¡
- **åŸºç¡€å¥–åŠ±**: +0.1 (é¼“åŠ±å­˜æ´»)
- **é€šè¿‡å¥–åŠ±**: +1.0 (ä¸»è¦ç›®æ ‡)
- **æ­»äº¡æƒ©ç½š**: -1.0 (é¿å…å±é™©)

### ğŸ”„ å­¦ä¹ è¿‡ç¨‹
1. **æ¢ç´¢**: å°è¯•ä¸åŒçš„è·³è·ƒç­–ç•¥
2. **å­¦ä¹ **: ä»æˆåŠŸå’Œå¤±è´¥ä¸­å­¦ä¹ 
3. **ä¼˜åŒ–**: é€æ¸æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥

è¿™ä¸ªä¼˜åŒ–åçš„DQNå®ç°äº†ç°ä»£å¼ºåŒ–å­¦ä¹ çš„æœ€ä½³å®è·µï¼Œåº”è¯¥èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ Flappy Birdæ¸¸æˆç­–ç•¥ã€‚ 