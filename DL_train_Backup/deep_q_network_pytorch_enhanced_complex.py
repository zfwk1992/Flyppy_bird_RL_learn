#!/usr/bin/env python
from __future__ import print_function

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import cv2
import sys
sys.path.append("game/")
import game.wrapped_flappy_bird_fast as game  # ä½¿ç”¨ä¼˜åŒ–æ¸¸æˆç‰ˆæœ¬
import random
import numpy as np
from collections import deque
import os
import logging
from datetime import datetime

# è®¾ç½®æ—¥å¿—è®°å½•
def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    # åˆ›å»ºlogsç›®å½•
    os.makedirs("logs", exist_ok=True)
    
    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼ˆåŒ…å«æ—¶é—´æˆ³ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"logs/training_enhanced_complex_{timestamp}.log"
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )
    
    logging.info(f"æ—¥å¿—æ–‡ä»¶åˆ›å»º: {log_filename}")
    return log_filename

# æ£€æŸ¥æ˜¯å¦æœ‰GPUå¯ç”¨
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# æ˜¾ç¤ºGPUè¯¦ç»†ä¿¡æ¯ï¼ˆåœ¨æ—¥å¿—é…ç½®åæ˜¾ç¤ºï¼‰
if torch.cuda.is_available():
    # è®¾ç½®GPUå†…å­˜åˆ†é…ç­–ç•¥
    torch.cuda.set_per_process_memory_fraction(0.8)  # ä½¿ç”¨80%çš„GPUå†…å­˜

# ğŸš€ å¢å¼ºå¤æ‚åº¦ç½‘ç»œé…ç½®å‚æ•° (é€šè¿‡æ¸¸æˆä¼˜åŒ–æå‡é€Ÿåº¦)
GAME = 'bird'
ACTIONS = 2
GAMMA = 0.99
OBSERVE = 3000        # é€‚åº¦è§‚å¯Ÿæ­¥æ•°
EXPLORE = 30000       # é€‚åº¦æ¢ç´¢æ­¥æ•°
FINAL_EPSILON = 0.001  # æœ€ç»ˆæ¢ç´¢ç‡
REPLAY_MEMORY = 25000  # è¾ƒå¤§ç»éªŒæ± 
BATCH = 32           # è¾ƒå¤§æ‰¹æ¬¡å¤§å°
FRAME_PER_ACTION = 2  # æ¯2å¸§ä¸€æ¬¡åŠ¨ä½œ (å¹³è¡¡é€Ÿåº¦å’Œè´¨é‡)

class EnhancedDQN(nn.Module):
    """å¢å¼ºå¤æ‚åº¦æ·±åº¦Qç½‘ç»œ"""
    def __init__(self, actions):
        super(EnhancedDQN, self).__init__()
        
        # å¢å¼ºå·ç§¯å±‚ - å¢åŠ é€šé“æ•°å’Œå±‚æ•°
        self.conv1 = nn.Conv2d(4, 64, kernel_size=8, stride=4)   # å¢åŠ é€šé“æ•°
        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2) # å¢åŠ é€šé“æ•°
        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=1)
        self.conv4 = nn.Conv2d(128, 64, kernel_size=3, stride=1) # æ–°å¢å·ç§¯å±‚
        
        # æ‰¹å½’ä¸€åŒ–å±‚
        self.bn1 = nn.BatchNorm2d(64)
        self.bn2 = nn.BatchNorm2d(128)
        self.bn3 = nn.BatchNorm2d(128)
        self.bn4 = nn.BatchNorm2d(64)
        
        # å¢å¼ºå…¨è¿æ¥å±‚ - å¢åŠ ç¥ç»å…ƒæ•°å’Œå±‚æ•°
        self.fc1 = nn.Linear(64, 1024)  # å¢åŠ ç¥ç»å…ƒæ•°
        self.fc2 = nn.Linear(1024, 512) # æ–°å¢éšè—å±‚
        self.fc3 = nn.Linear(512, 256)  # æ–°å¢éšè—å±‚
        self.fc4 = nn.Linear(256, actions)
        
        # æ‰¹å½’ä¸€åŒ–å±‚
        self.bn_fc1 = nn.BatchNorm1d(1024)
        self.bn_fc2 = nn.BatchNorm1d(512)
        self.bn_fc3 = nn.BatchNorm1d(256)
        
        # Dropoutå±‚
        self.dropout1 = nn.Dropout(0.2)
        self.dropout2 = nn.Dropout(0.2)
        self.dropout3 = nn.Dropout(0.1)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Conv2d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Linear):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm2d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm1d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        # å¢å¼ºå·ç§¯å±‚
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.max_pool2d(x, 2, 2)
        
        x = F.relu(self.bn2(self.conv2(x)))
        
        x = F.relu(self.bn3(self.conv3(x)))
        
        x = F.relu(self.bn4(self.conv4(x)))  # æ–°å¢å·ç§¯å±‚
        
        # å±•å¹³
        x = x.view(x.size(0), -1)  # å±•å¹³ä¸º64ç»´
        
        # å¢å¼ºå…¨è¿æ¥å±‚
        x = F.relu(self.bn_fc1(self.fc1(x)))
        x = self.dropout1(x)
        
        x = F.relu(self.bn_fc2(self.fc2(x)))
        x = self.dropout2(x)
        
        x = F.relu(self.bn_fc3(self.fc3(x)))
        x = self.dropout3(x)
        
        x = self.fc4(x)
        
        return x

class DQNAgent:
    """DQNæ™ºèƒ½ä½“"""
    def __init__(self, actions):
        self.actions = actions
        self.device = device
        
        # åˆ›å»ºå¢å¼ºç½‘ç»œ
        self.q_network = EnhancedDQN(actions).to(device)
        self.target_network = EnhancedDQN(actions).to(device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=1e-6, weight_decay=1e-5)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10000, gamma=0.9)
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.memory = deque(maxlen=REPLAY_MEMORY)
        
        # è®­ç»ƒå‚æ•°
        self.epsilon = 1.0  # åˆå§‹ä¸ºçº¯éšæœº
        self.step = 0
        
    def preprocess_state(self, state):
        """é¢„å¤„ç†çŠ¶æ€"""
        # è½¬æ¢ä¸ºç°åº¦å›¾
        state = cv2.cvtColor(cv2.resize(state, (80, 80)), cv2.COLOR_BGR2GRAY)
        # äºŒå€¼åŒ–
        _, state = cv2.threshold(state, 1, 255, cv2.THRESH_BINARY)
        return state
    
    def get_state_tensor(self, state_stack):
        """å°†çŠ¶æ€å †æ ˆè½¬æ¢ä¸ºtensor"""
        # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦
        state_tensor = torch.FloatTensor(state_stack).unsqueeze(0).to(device)
        # è°ƒæ•´ç»´åº¦é¡ºåº (batch, height, width, channels) -> (batch, channels, height, width)
        state_tensor = state_tensor.permute(0, 3, 1, 2)
        return state_tensor
    
    def select_action(self, state_tensor):
        """é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-è´ªå©ªç­–ç•¥ï¼‰"""
        if self.step <= OBSERVE:
            # è§‚å¯Ÿé˜¶æ®µï¼šçº¯éšæœºåŠ¨ä½œ
            action = random.randrange(self.actions)
            if self.step % 100 == 0:  # ä¿æŒæ—¥å¿—é¢‘ç‡
                logging.info(f"ğŸ‘€ è§‚å¯Ÿé˜¶æ®µéšæœºåŠ¨ä½œ: {action} (Îµ=1.0)")
        elif random.random() <= self.epsilon:
            # æ¢ç´¢é˜¶æ®µï¼šéšæœºåŠ¨ä½œ
            action = random.randrange(self.actions)
            if self.step % 100 == 0:  # ä¿æŒæ—¥å¿—é¢‘ç‡
                logging.info(f"ğŸ² æ¢ç´¢é˜¶æ®µéšæœºåŠ¨ä½œ: {action} (Îµ={self.epsilon:.4f})")
        else:
            # åˆ©ç”¨é˜¶æ®µï¼šç½‘ç»œå†³ç­–
            with torch.no_grad():
                q_values = self.q_network(state_tensor)
                action = q_values.argmax().item()
                if self.step % 100 == 0:  # ä¿æŒæ—¥å¿—é¢‘ç‡
                    logging.info(f"ğŸ§  ç½‘ç»œå†³ç­–: {action} (Qå€¼: {q_values.max().item():.4f})")
        
        return action
    
    def store_transition(self, state, action, reward, next_state, terminal):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.append((state, action, reward, next_state, terminal))
    
    def train(self):
        """è®­ç»ƒç½‘ç»œ"""
        if len(self.memory) < BATCH:
            return
        
        # éšæœºé‡‡æ ·æ‰¹æ¬¡
        batch = random.sample(self.memory, BATCH)
        
        # åˆ†ç¦»æ‰¹æ¬¡æ•°æ®å¹¶ç§»åˆ°GPU
        states = torch.FloatTensor([d[0] for d in batch]).to(device, non_blocking=True)
        actions = torch.LongTensor([d[1] for d in batch]).to(device, non_blocking=True)
        rewards = torch.FloatTensor([d[2] for d in batch]).to(device, non_blocking=True)
        next_states = torch.FloatTensor([d[3] for d in batch]).to(device, non_blocking=True)
        terminals = torch.BoolTensor([d[4] for d in batch]).to(device, non_blocking=True)
        
        # è°ƒæ•´ç»´åº¦é¡ºåº
        states = states.permute(0, 3, 1, 2)
        next_states = next_states.permute(0, 3, 1, 2)
        
        # è®¡ç®—å½“å‰Qå€¼
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (GAMMA * next_q_values * ~terminals)
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        # æ›´æ–°å­¦ä¹ ç‡
        self.scheduler.step()
        
        # æ¸…ç†GPUç¼“å­˜ï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return loss.item()
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def update_epsilon(self):
        """æ›´æ–°æ¢ç´¢ç‡"""
        if self.step <= OBSERVE:
            # è§‚å¯Ÿé˜¶æ®µï¼šä¿æŒçº¯éšæœº
            self.epsilon = 1.0
        elif self.epsilon > FINAL_EPSILON and self.step > OBSERVE:
            # æ¢ç´¢é˜¶æ®µï¼šä»1.0é€æ¸é™ä½åˆ°FINAL_EPSILON
            self.epsilon -= (1.0 - FINAL_EPSILON) / EXPLORE
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'epsilon': self.epsilon,
            'step': self.step
        }, path)
    
    def load_model(self, path):
        """åŠ è½½æ¨¡å‹"""
        if os.path.exists(path):
            checkpoint = torch.load(path, map_location=device)
            self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
            self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            self.epsilon = checkpoint['epsilon']
            self.step = checkpoint['step']
            logging.info(f"æˆåŠŸåŠ è½½æ¨¡å‹: {path}")
            return True
        else:
            logging.warning("æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹")
            return False

def train_network():
    """è®­ç»ƒç½‘ç»œ"""
    # è®¾ç½®æ—¥å¿—è®°å½•
    log_filename = setup_logging()
    logging.info("ğŸš€ å¼€å§‹Flappy Bird AIå¢å¼ºå¤æ‚åº¦ç½‘ç»œè®­ç»ƒ")
    logging.info("=" * 60)
    
    # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
    logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        logging.info(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
        logging.info(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        logging.info(f"å½“å‰GPUå†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated(0) / 1024**2:.1f} MB")
    else:
        logging.warning("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    
    # æ˜¾ç¤ºå¢å¼ºå¤æ‚åº¦ç½‘ç»œé…ç½®å‚æ•°
    logging.info("âš¡ å¢å¼ºå¤æ‚åº¦ç½‘ç»œé…ç½®å‚æ•°:")
    logging.info(f"   - FRAME_PER_ACTION: {FRAME_PER_ACTION}")
    logging.info(f"   - OBSERVE: {OBSERVE}")
    logging.info(f"   - EXPLORE: {EXPLORE}")
    logging.info(f"   - BATCH: {BATCH}")
    logging.info(f"   - ç½‘ç»œå¤æ‚åº¦: å¢å¼ºç‰ˆ (æ›´å¤šå±‚ã€æ›´å¤šç¥ç»å…ƒã€æ‰¹å½’ä¸€åŒ–ã€Dropout)")
    logging.info(f"   - é¢„æœŸé€Ÿåº¦: ~30æ­¥/ç§’ (é€šè¿‡æ¸¸æˆä¼˜åŒ–)")
    logging.info(f"   - é¢„æœŸæ—¶é—´: ~18åˆ†é’Ÿ")
    logging.info(f"   - ä½¿ç”¨ä¼˜åŒ–æ¸¸æˆç‰ˆæœ¬")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = DQNAgent(ACTIONS)
    
    # éªŒè¯æ¨¡å‹æ˜¯å¦åœ¨GPUä¸Š
    if torch.cuda.is_available():
        q_network_device = next(agent.q_network.parameters()).device
        target_network_device = next(agent.target_network.parameters()).device
        logging.info(f"Qç½‘ç»œè®¾å¤‡: {q_network_device}")
        logging.info(f"ç›®æ ‡ç½‘ç»œè®¾å¤‡: {target_network_device}")
        
        if q_network_device.type != 'cuda' or target_network_device.type != 'cuda':
            logging.error("âŒ æ¨¡å‹æœªæ­£ç¡®ç§»åˆ°GPUï¼")
            # å¼ºåˆ¶ç§»åˆ°GPU
            agent.q_network = agent.q_network.to(device)
            agent.target_network = agent.target_network.to(device)
            logging.info("âœ… å·²å¼ºåˆ¶å°†æ¨¡å‹ç§»åˆ°GPU")
    
    # åˆ›å»ºæ¸¸æˆç¯å¢ƒ
    game_state = game.GameState()
    
    # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
    os.makedirs(f"logs_{GAME}", exist_ok=True)
    a_file = open(f"logs_{GAME}/readout.txt", 'w')
    h_file = open(f"logs_{GAME}/hidden.txt", 'w')
    
    # åˆå§‹åŒ–çŠ¶æ€
    do_nothing = np.zeros(ACTIONS)
    do_nothing[0] = 1
    x_t, r_0, terminal = game_state.frame_step(do_nothing)
    x_t = agent.preprocess_state(x_t)
    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)
    
    # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model_path = "saved_networks/bird-dqn-pytorch-enhanced-complex.pth"
    agent.load_model(model_path)
    
    # å¸§è®¡æ•°å™¨
    frame_count = 0
    
    # è®­ç»ƒå¾ªç¯
    while "flappy bird" != "angry bird":
        # è·å–çŠ¶æ€tensor
        state_tensor = agent.get_state_tensor(s_t)
        
        # æ¯FRAME_PER_ACTIONå¸§é‡‡å–ä¸€æ¬¡åŠ¨ä½œ
        if frame_count % FRAME_PER_ACTION == 0:
            # é€‰æ‹©åŠ¨ä½œ
            action_index = agent.select_action(state_tensor)
            a_t = np.zeros([ACTIONS])
            a_t[action_index] = 1
        else:
            # å…¶ä»–å¸§ä¿æŒä¸Šä¸€ä¸ªåŠ¨ä½œ
            a_t = np.zeros([ACTIONS])
            a_t[action_index] = 1
        
        # æ‰§è¡ŒåŠ¨ä½œ
        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)
        x_t1 = agent.preprocess_state(x_t1_colored)
        x_t1 = np.reshape(x_t1, (80, 80, 1))
        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)
        
        # å­˜å‚¨ç»éªŒï¼ˆåªåœ¨é‡‡å–åŠ¨ä½œçš„å¸§å­˜å‚¨ï¼‰
        if frame_count % FRAME_PER_ACTION == 0:
            agent.store_transition(s_t, action_index, r_t, s_t1, terminal)
            agent.step += 1
        
        # æ›´æ–°çŠ¶æ€
        s_t = s_t1
        frame_count += 1
        
        # åªåœ¨é‡‡å–åŠ¨ä½œçš„å¸§è¿›è¡Œè®­ç»ƒå’Œæ—¥å¿—è®°å½•
        if frame_count % FRAME_PER_ACTION == 0:
            # è®­ç»ƒç½‘ç»œ
            loss = None
            if agent.step > OBSERVE:
                loss = agent.train()
            
            # æ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆæ¯5000æ­¥ï¼‰
            if agent.step % 5000 == 0:
                agent.update_target_network()
            
            # æ›´æ–°æ¢ç´¢ç‡
            agent.update_epsilon()
            
            # ä¿å­˜æ¨¡å‹ï¼ˆæ¯5000æ­¥ï¼‰
            if agent.step % 5000 == 0:
                os.makedirs("saved_networks", exist_ok=True)
                agent.save_model(f"saved_networks/{GAME}-dqn-pytorch-enhanced-complex-{agent.step}.pth")
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯ï¼ˆä¿æŒæ—¥å¿—é¢‘ç‡ï¼‰
            if agent.step % 100 == 0:
                state = ""
                if agent.step <= OBSERVE:
                    state = "observe"
                elif agent.step > OBSERVE and agent.step <= OBSERVE + EXPLORE:
                    state = "explore"
                else:
                    state = "train"
                
                q_max = agent.q_network(state_tensor).max().item() if loss is not None else 0
                current_lr = agent.scheduler.get_last_lr()[0]
                
                logging.info(f"â±ï¸  TIMESTEP {agent.step} / FRAME {frame_count} / STATE {state} / EPSILON {agent.epsilon:.4f} / "
                      f"ACTION {action_index} / REWARD {r_t} / Q_MAX {q_max:.4f} / LR {current_lr:.2e}")
                
                if loss is not None:
                    logging.info(f"ğŸ“‰ LOSS: {loss:.6f}")
            
            # æ¯1000æ­¥æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if agent.step % 1000 == 0:
                logging.info(f"ğŸ“Š ç»Ÿè®¡: ç»éªŒæ± å¤§å°={len(agent.memory)}, æ¢ç´¢ç‡={agent.epsilon:.4f}")
                
                # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.memory_allocated(0) / 1024**2
                    gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    gpu_memory_reserved = torch.cuda.memory_reserved(0) / 1024**2
                    logging.info(f"ğŸ–¥ï¸  GPUå†…å­˜: {gpu_memory:.1f} MB / {gpu_memory_reserved:.1f} MB / {gpu_memory_total:.1f} GB")
                    
                    # æ£€æŸ¥æ¨¡å‹å‚æ•°æ˜¯å¦åœ¨GPUä¸Š
                    q_network_device = next(agent.q_network.parameters()).device
                    logging.info(f"ğŸ” Qç½‘ç»œè®¾å¤‡: {q_network_device}")
                
                logging.info("-" * 50)

def play_game():
    """ä¸»å‡½æ•°"""
    train_network()

if __name__ == "__main__":
    play_game() 